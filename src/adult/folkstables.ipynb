{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models import ResNetClassifier\n",
    "\n",
    "from divexp import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def closest_odd(n):\n",
    "    n = int(n)\n",
    "    if n % 2 == 0:\n",
    "        return n + 1\n",
    "    else:\n",
    "        return n\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = CelebA(root='./data', split='all', download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    argParser = argparse.ArgumentParser()\n",
    "    argParser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "    argParser.add_argument(\"--checkpoint\", type=str)\n",
    "    argParser.add_argument(\"--n-targets\", type=int, default=100)\n",
    "    argParser.add_argument(\"--start-noise\", type=int, default=10) # add noise after 10 batches\n",
    "    argParser.add_argument(\"--frac-noise\", type=float, default=1.0) # % of points to add noise to\n",
    "    argParser.add_argument(\"--batch-size\", type=int, default=1024) # % of points to add noise to\n",
    "    argParser.add_argument(\"--metric\", type=str, choices=[\"accuracy\"], default=\"accuracy\") # add noise after 10 batches\n",
    "\n",
    "    args = argParser.parse_args()\n",
    "\n",
    "    ckpt_dir = \"models-ckpt\"\n",
    "    pt_filename = os.path.join(ckpt_dir, f\"{args.checkpoint}.pt\")\n",
    "    json_filename = os.path.join(ckpt_dir, f\"{args.checkpoint}.json\")\n",
    "    ds_filename = os.path.join(ckpt_dir, f\"{args.checkpoint}.pkl\")\n",
    "    matches_filename = os.path.join(ckpt_dir, f\"matches-{args.checkpoint}.pkl\")\n",
    "\n",
    "    model = torch.load(pt_filename).to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    with open(json_filename, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    with open(ds_filename, \"rb\") as f:\n",
    "        ds = pickle.load(f)\n",
    "        train_set, test_set, test_sets = ds[\"train\"], ds[\"test\"], ds[\"test_chunks\"]\n",
    "    \n",
    "    with open(matches_filename, \"rb\") as f:\n",
    "        matches_obj = pickle.load(f)\n",
    "        df_train = matches_obj[\"metadata_train\"]\n",
    "        df_tests = matches_obj[\"metadata_batches\"]\n",
    "        matches = matches_obj[\"matches_train\"]\n",
    "        matches_ts_list = matches_obj[\"matches_batches\"]\n",
    "        col_names = matches_obj[\"col_names\"]\n",
    "        metadata_mask = matches_obj[\"metadata_mask\"]\n",
    "        metadata_names = matches_obj[\"metadata_names\"]\n",
    "    \n",
    "    attr_id = config[\"attr_id\"]\n",
    "    output_dir = os.path.join(ckpt_dir, f\"{args.checkpoint}-{args.metric}-noise-{args.frac_noise:.2f}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for target_sg in matches.fi.itemsets.sample(n=args.n_targets):\n",
    "        sg = tuple(target_sg)\n",
    "        outfile = os.path.join(output_dir, f\"target-{'-'.join(map(str, sorted(sg)))}.pkl\")\n",
    "        print(\"Target\", sg, \"output\", outfile)\n",
    "\n",
    "        n_batches = len(test_sets)\n",
    "        blurs = [None] * args.start_noise + [\n",
    "            transforms.GaussianBlur(closest_odd(i), sigma=i) if i > 0 else lambda x : x # identity if sigma=0\n",
    "            for i in np.linspace(1, 40, n_batches - args.start_noise)\n",
    "        ]\n",
    "\n",
    "        accuracies = []\n",
    "        f1 = []\n",
    "        y_trues = []\n",
    "        y_preds = []\n",
    "        divs = []\n",
    "\n",
    "        samples = [None] * len(test_sets)\n",
    "\n",
    "        for pos, (ts, blur, matches_ts) in enumerate(zip(test_sets, blurs, matches_ts_list)):\n",
    "            # get predictions\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "                metadata = []\n",
    "\n",
    "                dl = DataLoader(ts, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "                md = np.vstack([ y[:, metadata_mask].numpy() for _, y in dl ])\n",
    "                in_target = (md[:, sg] == 1).all(axis=1).nonzero()[0]\n",
    "                mask = torch.zeros(len(md)).bool()\n",
    "                picked = np.random.choice(in_target, size=int(round(len(in_target) * args.frac_noise)), replace=False)\n",
    "                mask[picked] = True\n",
    "                dl_mask = DataLoader(TensorDataset(mask), batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "                tot_target = len(in_target)\n",
    "\n",
    "\n",
    "                num_altered = 0\n",
    "                for (x, y), (mask_batch,) in tqdm(zip(dl, dl_mask), total=len(dl)):\n",
    "                    md = y[:, metadata_mask].numpy().astype(int)\n",
    "\n",
    "                    # apply blur, if required\n",
    "                    # if blur is not None:\n",
    "                    #     # only add noise to points in target subgroups\n",
    "                    #     # mask = (md[:, sg] == 1).all(axis=1)\n",
    "                    #     if mask.any():\n",
    "                    #         num_altered += mask.sum()\n",
    "                    #         x[mask] = blur(x[mask])\n",
    "                    if blur is not None and mask_batch.any():\n",
    "                        num_altered += mask_batch.sum().item()\n",
    "                        x[mask_batch] = blur(x[mask_batch])\n",
    "                    \n",
    "                    if mask_batch.any() and samples[pos] is None:\n",
    "                        samples[pos] = x[mask_batch][0].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "                    out = model(x.to(args.device))\n",
    "                    preds = (out > 0).cpu().numpy().astype(int)\n",
    "                    y_pred.extend(preds)\n",
    "                    y_true.extend(y[:, attr_id].numpy().astype(int))\n",
    "                    metadata.extend(md)\n",
    "                \n",
    "                y_true = np.hstack(y_true)\n",
    "                y_pred = np.hstack(y_pred)\n",
    "                metadata = np.vstack(metadata)\n",
    "\n",
    "                y_trues.append(y_true)\n",
    "                y_preds.append(y_pred)\n",
    "\n",
    "                divs.append(div_explorer(matches_ts, y_true, y_pred, [args.metric]))\n",
    "\n",
    "                accuracies.append(accuracy_score(y_true, y_pred))\n",
    "                f1.append(f1_score(y_true, y_pred))\n",
    "                print(\"Altered\", num_altered, \"out of\", tot_target, \"accuracy\", accuracies[-1], \"f1\", f1[-1])\n",
    "        \n",
    "        # store results\n",
    "\n",
    "        fig, ax = plt.subplots(1, len(samples), figsize=(len(samples) * 3, 3))\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax[i].imshow(sample)\n",
    "            ax[i].axis('off')\n",
    "        fig.savefig(os.path.join(output_dir, f\"target-{'-'.join(map(str, sorted(sg)))}.png\"))\n",
    "        \n",
    "        with open(outfile, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"subgroup\": sg,\n",
    "                \"accuracies\": accuracies,\n",
    "                \"f1\": f1,\n",
    "                \"divs\": divs,\n",
    "                \"y_trues\": y_trues,\n",
    "                \"y_preds\": y_preds,\n",
    "                \"blurs\": blurs,\n",
    "            }, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
